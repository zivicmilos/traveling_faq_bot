{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14082bf",
   "metadata": {},
   "source": [
    "# Custom Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a2f8d",
   "metadata": {},
   "source": [
    "## Custom word vectors: sum and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b44c8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "N_NEIGHBOURS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "871e7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads dataset\n",
    "\n",
    "    :param path:\n",
    "        dataset path\n",
    "    :return:\n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df.drop(columns=df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d02c41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(\n",
    "    wv: KeyedVectors, document: list[str], tfidf_vectorizer: TfidfVectorizer = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transforms documents to vectors\n",
    "\n",
    "    :param wv: KeyedVectors\n",
    "        vectors of all words from vocabulary\n",
    "    :param document:\n",
    "        input document from corpus\n",
    "    :param tfidf_vectorizer: TfidfVectorizer\n",
    "        TF-IDF vectorizer\n",
    "    :return:\n",
    "        vector representation of question\n",
    "    \"\"\"\n",
    "    if SENTENCE_VECTOR_WEIGHT == \"idf\":\n",
    "        idf = np.asarray(\n",
    "            [\n",
    "                tfidf_vectorizer.idf_[tfidf_vectorizer.vocabulary_[token]]\n",
    "                for token in document\n",
    "            ]\n",
    "        )\n",
    "        document = np.asarray([wv[token] for token in document])\n",
    "        document = idf[:, np.newaxis] * document\n",
    "    elif SENTENCE_VECTOR_WEIGHT == \"pos\":\n",
    "        doc = nlp(\" \".join(document))\n",
    "        pos = np.asarray([POS.get(token.pos_, 1.0) for token in doc])\n",
    "\n",
    "        document = np.asarray([wv[token] for token in document])\n",
    "        document = pos[:, np.newaxis] * document\n",
    "    elif SENTENCE_VECTOR_WEIGHT == \"ner\":\n",
    "        doc = nlp(\" \".join(document))\n",
    "        ner = np.asarray([NER.get(token.ent_type_, 1.0) for token in doc])\n",
    "\n",
    "        document = np.asarray([wv[token] for token in document])\n",
    "        document = ner[:, np.newaxis] * document\n",
    "    elif SENTENCE_VECTOR_WEIGHT == \"pos+ner\":\n",
    "        doc = nlp(\" \".join(document))\n",
    "        pos = np.asarray([POS.get(token.pos_, 1.0) for token in doc])\n",
    "        ner = np.asarray([NER.get(token.ent_type_, 1.0) for token in doc])\n",
    "        pos_ner = pos + ner\n",
    "\n",
    "        document = np.asarray([wv[token] for token in document])\n",
    "        document = pos_ner[:, np.newaxis] * document\n",
    "    else:\n",
    "        document = np.asarray([wv[token] for token in document])\n",
    "\n",
    "    if SENTENCE_VECTOR_STRATEGY == \"sum\":\n",
    "        document = np.sum(document, axis=0)\n",
    "    elif SENTENCE_VECTOR_STRATEGY == \"average\":\n",
    "        document = np.mean(document, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Strategy {SENTENCE_VECTOR_STRATEGY} is not supported. Try 'sum' or 'average'\"\n",
    "        )\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0de18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(\n",
    "    wv: KeyedVectors,\n",
    "    knn: NearestNeighbors,\n",
    "    corpus: list[list[str]],\n",
    "    tfidf_vectorizer: TfidfVectorizer = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate performance of finding similar questions\n",
    "\n",
    "    :param wv: KeyedVectors\n",
    "        vectors of all words from vocabulary\n",
    "    :param knn: NearestNeighbors\n",
    "        K-nearest neighbors\n",
    "    :param corpus: list\n",
    "        input corpus of documents\n",
    "    :param tfidf_vectorizer: TfidfVectorizer\n",
    "        TF-IDF vectorizer\n",
    "    :return:\n",
    "        score (lesser is better)\n",
    "    \"\"\"\n",
    "    with open(\"../../../data/test_questions_json.json\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    test_questions = json_data[\"question\"]\n",
    "    original = json_data[\"original\"]\n",
    "\n",
    "    test_questions = [list(tokenize(tq.lower())) for tq in test_questions]\n",
    "    for i, tq in enumerate(test_questions):\n",
    "        test_questions[i] = list(filter(lambda x: x in wv.index_to_key, tq))\n",
    "    test_questions = np.asarray(\n",
    "        [vectorize(wv, tq, tfidf_vectorizer) for tq in test_questions]\n",
    "    )\n",
    "    _, indices = knn.kneighbors(test_questions)\n",
    "\n",
    "    original = [list(tokenize(o.lower())) for o in original]\n",
    "    indices_original = np.asarray([corpus.index(o) for o in original])\n",
    "\n",
    "    rank = np.where(indices == indices_original[:, None])[1]\n",
    "    penalization = (indices_original.shape[0] - rank.shape[0]) * 2 * knn.n_neighbors\n",
    "    score = (rank.sum() + penalization) / indices_original.shape[0]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6de74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../../../data/insurance_qna_dataset.csv\")\n",
    "\n",
    "questions = np.unique(df.iloc[:, 0].to_numpy())\n",
    "questions = [list(tokenize(question.lower())) for question in questions]\n",
    "\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5dc6b",
   "metadata": {},
   "source": [
    "### Custom word vectors: sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d01227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.13%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"sum\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"none\"\n",
    "\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b9462",
   "metadata": {},
   "source": [
    "### Custom word vectors: average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80f4c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.13%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"average\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"none\"\n",
    "\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508d5c8",
   "metadata": {},
   "source": [
    "## Custom word vectors combined with IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79cfa71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16896x3631 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 122954 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "tfidf_vectorizer.fit_transform([\" \".join(question) for question in questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38154069",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with IDF: sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee142b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 69.44%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"sum\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"idf\"\n",
    "\n",
    "vectorized_questions = np.asarray([vectorize(wv, question, tfidf_vectorizer) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions, tfidf_vectorizer)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38728155",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with IDF: average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a93a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 69.44%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"average\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"idf\"\n",
    "\n",
    "vectorized_questions = np.asarray([vectorize(wv, question, tfidf_vectorizer) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions, tfidf_vectorizer)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bfa10",
   "metadata": {},
   "source": [
    "## Custom word vectors combined with POS/NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "752b89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r\"\\S+\").match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c441a",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with POS: sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f706991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.30%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"sum\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"pos\"\n",
    "\n",
    "POS = {\"NOUN\": 5.0, \"PROPN\": 6.0, \"VERB\": 2.0, \"ADJ\": 4.0}\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786f662",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with POS: average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "498d0373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.30%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"average\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"pos\"\n",
    "\n",
    "POS = {\"NOUN\": 5.0, \"PROPN\": 6.0, \"VERB\": 2.0, \"ADJ\": 4.0}\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94232e",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with NER: sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1065de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.12%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"sum\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"ner\"\n",
    "\n",
    "NER = {\"MONEY\": 6.0, \"CARDINAL\": 5.0, \"DATE \": 4.0, \"FAC \": 4.0}\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e83b5d",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with NER: average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "90ffa758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.12%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"average\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"ner\"\n",
    "\n",
    "NER = {\"MONEY\": 6.0, \"CARDINAL\": 5.0, \"DATE \": 4.0, \"FAC \": 4.0}\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57032c6d",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with POS+NER: sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1bb412ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 78.88%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"sum\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"pos+ner\"\n",
    "\n",
    "POS = {\"NOUN\": 5.0, \"PROPN\": 6.0, \"VERB\": 2.0, \"ADJ\": 4.0}\n",
    "NER = {\"MONEY\": 6.0, \"CARDINAL\": 5.0, \"DATE \": 4.0, \"FAC \": 4.0}\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f3dd8",
   "metadata": {},
   "source": [
    "### Custom word vectors combined with POS+NER: average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc94717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 78.88%\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_VECTOR_STRATEGY = \"average\"\n",
    "SENTENCE_VECTOR_WEIGHT = \"pos+ner\"\n",
    "\n",
    "POS = {\"NOUN\": 5.0, \"PROPN\": 6.0, \"VERB\": 2.0, \"ADJ\": 4.0}\n",
    "NER = {\"MONEY\": 6.0, \"CARDINAL\": 5.0, \"DATE \": 4.0, \"FAC \": 4.0}\n",
    "vectorized_questions = np.asarray([vectorize(wv, question) for question in questions])\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(wv, knn, questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:synechron] *",
   "language": "python",
   "name": "conda-env-synechron-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
