{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d24fbe",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e962139",
   "metadata": {},
   "source": [
    "## Term Frequency with different distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3fa95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "N_NEIGHBOURS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af380c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(\n",
    "    documents: Iterable[str], preprocessing: str, stemmer: str = \"porter\"\n",
    ") -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    Applies preprocessing to iterable of documents\n",
    "\n",
    "    :param documents: Iterable[str]\n",
    "        iterable of documents\n",
    "    :param preprocessing: str\n",
    "        represents type of preprocessing applied to documents\n",
    "    :param stemmer: str\n",
    "        stemmer type\n",
    "    :return:\n",
    "        processed iterable of documents\n",
    "    \"\"\"\n",
    "    if preprocessing == \"stemming\":\n",
    "        documents = np.asarray(\n",
    "            [stem_document(document, stemmer) for document in documents]\n",
    "        )\n",
    "    elif preprocessing == \"lemmatizing\":\n",
    "        documents = np.asarray([lemmatize_document(document) for document in documents])\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db91f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(\n",
    "    vectorizer: CountVectorizer,\n",
    "    knn: NearestNeighbors,\n",
    "    vectorized_questions: np.ndarray,\n",
    "    preprocessing: str = None,\n",
    "    stemmer: str = \"porter\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate performance of finding similar questions\n",
    "\n",
    "    :param vectorizer: CountVectorizer\n",
    "        term frequency vectorizer\n",
    "    :param knn: NearestNeighbors\n",
    "        K-nearest neighbors\n",
    "    :param vectorized_questions: np.ndarray\n",
    "        input questions transformed with count vectorizer\n",
    "    :param preprocessing: str\n",
    "        represents type of preprocessing applied to documents\n",
    "    :param stemmer: str\n",
    "        stemmer type\n",
    "    :return:\n",
    "        score (lesser is better)\n",
    "    \"\"\"\n",
    "    with open(\"../../../data/test_questions_json.json\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    test_questions = json_data[\"question\"]\n",
    "    original = json_data[\"original\"]\n",
    "\n",
    "    test_questions = preprocess_documents(test_questions, preprocessing, stemmer)\n",
    "    test_questions = vectorizer.transform(test_questions)\n",
    "    _, indices = knn.kneighbors(test_questions.toarray())\n",
    "\n",
    "    original = preprocess_documents(original, preprocessing, stemmer)\n",
    "    original = vectorizer.transform(original)\n",
    "\n",
    "    original = list(map(set, vectorizer.inverse_transform(original)))\n",
    "    vectorized_questions = list(\n",
    "        map(set, vectorizer.inverse_transform(vectorized_questions))\n",
    "    )\n",
    "\n",
    "    indices_original = np.asarray([vectorized_questions.index(o) for o in original])\n",
    "\n",
    "    rank = np.where(indices == indices_original[:, None])[1]\n",
    "    penalization = (indices_original.shape[0] - rank.shape[0]) * 2 * knn.n_neighbors\n",
    "    score = (rank.sum() + penalization) / indices_original.shape[0]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39137dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../data/traveling_qna_dataset.csv\", sep=\"\\t\")\n",
    "df.drop(columns=df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "questions = df.iloc[:, 0].to_numpy()\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068caf3e",
   "metadata": {},
   "source": [
    "### Euclidean metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f88950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 92.43%\n"
     ]
    }
   ],
   "source": [
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"euclidean\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382791b8",
   "metadata": {},
   "source": [
    "### Manhattan (cityblock) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f09537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 92.19%\n"
     ]
    }
   ],
   "source": [
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cityblock\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36088f22",
   "metadata": {},
   "source": [
    "### Cosine metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a9135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 96.45%\n"
     ]
    }
   ],
   "source": [
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca26794",
   "metadata": {},
   "source": [
    "## Comparison between different numbers of neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182d74a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbours: 1 | Score: 93.33%\n",
      "Number of neighbours: 5 | Score: 94.83%\n",
      "Number of neighbours: 10 | Score: 94.92%\n",
      "Number of neighbours: 25 | Score: 94.97%\n",
      "Number of neighbours: 50 | Score: 96.20%\n",
      "Number of neighbours: 100 | Score: 96.45%\n",
      "Number of neighbours: 150 | Score: 97.36%\n",
      "Number of neighbours: 200 | Score: 97.59%\n",
      "Number of neighbours: 300 | Score: 97.83%\n",
      "Number of neighbours: 400 | Score: 98.90%\n",
      "Number of neighbours: 500 | Score: 99.15%\n",
      "Number of neighbours: 1000 | Score: 99.57%\n",
      "Number of neighbours: 2000 | Score: 99.79%\n",
      "Number of neighbours: 3000 | Score: 99.86%\n"
     ]
    }
   ],
   "source": [
    "N_NEIGHBOURS_GRID = (1, 5, 10, 25, 50, 100, 150, 200, 300, 400, 500, 1000, 2000, 3000)\n",
    "for N_NEIGHBOURS in N_NEIGHBOURS_GRID:\n",
    "    knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "    score = check_performance(vectorizer, knn, vectorized_questions)\n",
    "    print(f\"Number of neighbours: {N_NEIGHBOURS} | Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")\n",
    "N_NEIGHBOURS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69dd7f",
   "metadata": {},
   "source": [
    "## Term Frequency with Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75df13ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 96.67%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "\n",
    "questions = df.iloc[:, 0].to_numpy()\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5e8d4",
   "metadata": {},
   "source": [
    "## Term Frequency with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae8b978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 96.27%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "questions = df.iloc[:, 0].to_numpy()\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions)\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0ff04",
   "metadata": {},
   "source": [
    "## Term Frequency with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "242e446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_document(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatize the input document and return processed document\n",
    "\n",
    "    :param document: str\n",
    "        document to be lemmatized\n",
    "    :return:\n",
    "        lemmatized document\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(document)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5c9766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 96.42%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "\n",
    "questions = df.iloc[:, 0].to_numpy()\n",
    "questions = preprocess_documents(questions, \"lemmatizing\")\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions, \"lemmatizing\")\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34148843",
   "metadata": {},
   "source": [
    "## Term Frequency with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08fb0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_document(document: str, stemmer: str = \"porter\") -> str:\n",
    "    \"\"\"\n",
    "    Stem the input document and return processed document\n",
    "\n",
    "    :param document: str\n",
    "        document to be stemmed\n",
    "    :param stemmer: str\n",
    "        stemmer type\n",
    "    :return:\n",
    "        stemmed document\n",
    "    \"\"\"\n",
    "    if stemmer == \"porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer == \"snowball\":\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    elif stemmer == \"lancaster\":\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Stemmer type '{stemmer}' is not supported. Try with 'porter', 'snowball' or 'lancaster'.\"\n",
    "        )\n",
    "    tokens = word_tokenize(document)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc3c0f",
   "metadata": {},
   "source": [
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "855e96aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 97.78%\n"
     ]
    }
   ],
   "source": [
    "questions = df.iloc[:, 0].to_numpy()\n",
    "questions = preprocess_documents(questions, \"stemming\", \"porter\")\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions, \"stemming\", \"porter\")\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1dcf1",
   "metadata": {},
   "source": [
    "### Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0558d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 97.78%\n"
     ]
    }
   ],
   "source": [
    "questions = df.iloc[:, 0].to_numpy()\n",
    "questions = preprocess_documents(questions, \"stemming\", \"snowball\")\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions, \"stemming\", \"snowball\")\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b626217",
   "metadata": {},
   "source": [
    "### Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e8b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 97.81%\n"
     ]
    }
   ],
   "source": [
    "questions = df.iloc[:, 0].to_numpy()\n",
    "questions = preprocess_documents(questions, \"stemming\", \"lancaster\")\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions, \"stemming\", \"lancaster\")\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fea30",
   "metadata": {},
   "source": [
    "## Term Frequency with Stemming and Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10d9c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 98.31%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 1), stop_words=\"english\")\n",
    "\n",
    "questions = df.iloc[:, 0].to_numpy()\n",
    "questions = preprocess_documents(questions, \"stemming\", \"snowball\")\n",
    "vectorized_questions = vectorizer.fit_transform(questions)\n",
    "vectorized_questions = np.unique(vectorized_questions.toarray(), axis=0)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=N_NEIGHBOURS, metric=\"cosine\").fit(vectorized_questions)\n",
    "\n",
    "score = check_performance(vectorizer, knn, vectorized_questions, \"stemming\", \"snowball\")\n",
    "print(f\"Score: {100 - score / (2 * N_NEIGHBOURS) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:synechron]",
   "language": "python",
   "name": "conda-env-synechron-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
